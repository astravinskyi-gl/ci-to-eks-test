name: Docker Image CI

on:
  workflow_dispatch:
  pull_request:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest

# permissions required by configure AWS creds step
    permissions:
      id-token: write
      contents: write

    env:
      DOCKER_USER: ${{ secrets.DOCKER_USER }}
      DOCKER_HUB_REPO: demoapp
      AWS_REGION: us-east-2
      AWS_ROLE: arn:aws:iam::331226338131:role/ci-GithubActionsRole
    
    steps:
      - name: Checkout
        uses: actions/checkout@v3

#       - name: Check file
#         run: cat hello.py
        
      - name: Checkout config repo
        uses: actions/checkout@v3
        with:
          repository: astravinskyi-gl/demo-app
          path: mec_kubernetes

#       - name: Add semantic version
#         id: version
#         uses: PaulHatch/semantic-version@v4.0.3
#         with:
#           branch: master
#           tag_prefix: "v"
#           major_pattern: "BREAKING CHANGE:"
#           minor_pattern: "feat:"
#           format: "v${major}.${minor}.${patch}"

      - name: Login to DockerHub
        uses: docker/login-action@v1
        with:
          username: ${{ env.DOCKER_USER }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build and push
        uses: docker/build-push-action@v2
        with:
          push: true
          #tags: ${{ env.DOCKER_USER }}/${{ env.DOCKER_HUB_REPO }}:latest, ${{ env.DOCKER_USER }}/${{ env.DOCKER_HUB_REPO }}:${{ steps.version.outputs.version }}
          tags: ${{ env.DOCKER_USER }}/${{ env.DOCKER_HUB_REPO }}:rc
      
#       - name: Create Release
#         if: ${{ !startsWith(github.ref, 'refs/tags/') }}
#         uses: actions/create-release@v1
#         env:
#           GITHUB_TOKEN: ${{ secrets.github_token }}
#         with:
#           tag_name: ${{ steps.version.outputs.version }}
#           release_name: ${{ steps.version.outputs.version }}

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          role-to-assume: ${{ env.AWS_ROLE }}
          #role-session-name: k8s-job-gh-actions
          aws-region: ${{ env.AWS_REGION }}
          
      - name: Setup kubectl
        uses: azure/setup-kubectl@v2.0
        with:
          version: 'v1.23.5'
          
      - name: Set Kubernetes Context
        uses: azure/k8s-set-context@v1
        with:
          method: kubeconfig
          kubeconfig: ${{ secrets.KUBECONFIG }}

      - name: Install Helm
        uses: azure/setup-helm@v1
        with:
          version: v3.8.0
          
      - name: test kubectl
        run: kubectl get pods

### Alternative method which is better but requires adding additional labels to deployment
### https://stackoverflow.com/questions/53450759/helm-upgrade-install-isnt-picking-up-new-changes
#       - name: Get current image tag in release
#         run: |
#           echo "CURRENT_TAG=$(helm get values myapp | sed -n -e 's/^.*tag: //p')" >> $GITHUB_ENV
        
      - name: Deploy
        run: helm upgrade --install --set image.repository="${{ env.DOCKER_USER }}/${{ env.DOCKER_HUB_REPO }}" --set image.tag=rc myapp testchart
      
      - name: Run test job
        id: test_job
        run: kubectl replace --force -f mec_kubernetes/test-job.yml
        
      - name: test set var
        run: echo "action_state=yellow" >> $GITHUB_ENV
        
      - name: Get test job pod name
        run: |
          E2E_POD=$(kubectl get pods --selector=job-name=mmg-e2etest-job --sort-by=.status.startTime | sed -n '$p' | awk '{print $1}')
          echo $E2E_POD >> GITHUB_ENV
          
      - name: test var
        run: echo ${{ env.E2E_POD }}
          
      - name: Check job status
        timeout-minutes: 5
        run: |
          JOB_STATUS=$(kubectl get pods ${{ env.E2E_POD }} --no-headers -o custom-columns=':status.phase')
          while [ $JOB_STATUS != "Succeeded" ]
          do
            JOB_STATUS=$(kubectl get pods ${{ env.E2E_POD }} --no-headers -o custom-columns=':status.phase')
          done
      
      - name: Get test results
        run: |
          TEST_FAILED=$(kubectl logs ${{ env.E2E_POD }} | grep 'All Test Done' | sed -e 's/.*total fail \(.\).*\*\*\*/\1/')
          echo $TEST_FAILED >> GITHUB_ENV
      
      - name: Rollback to previous version
        if: steps.test_job.outputs.exit_code == 0
        run: |
          #helm upgrade --install --set image.repository="${{ env.DOCKER_USER }}/${{ env.DOCKER_HUB_REPO }}" --set image.tag=${{ env.CURRENT_TAG }} myapp testchart
          helm rollback myapp 0
          
      - name: Show e2e-test result
        if: ${{ env.TEST_FAILED }} != 0
        run: |
          echo "tests failed: " ${{ env.TEST_FAILED }}
          echo "Complete logs output:" && kubectl logs ${{ env.E2E_POD }}
          
    
    ### to get job status
    # kubectl get pods --selector=job-name=mmg-e2etest-job --sort-by=.status.startTime | sed -n '$p' | awk '{print $3}'

    # kubectl logs $(kubectl get pods --selector=job-name=mmg-e2etest-job --sort-by=.status.startTime | sed -n '$p' | awk '{print $1}')
    
    # kubectl logs $(kubectl get pods --selector=job-name=mmg-e2etest-job --sort-by=.status.startTime | sed -n '$p' | awk '{print $1}') | grep 'All Test Done' | sed -e 's/.*total succ \(.*\)total fail.*/\1/' 8
    
    # | grep 'All Test Done' | sed -e 's/.*total fail \(.\).*\*\*\*/\1/'
    
    # kubectl get pods mmg-e2etest-job-tf2bz --no-headers -o custom-columns=":status.phase"
